layout: page
title: "Literature Review"
permalink: /literature-review/

# Project Literature Review
Jenna Brandt and Erin Puckett

## Four related pieces of work:
- [Still Out There: Modeling and Identifying Russian Troll Accounts on Twitter](https://arxiv.org/pdf/1901.11162.pdf): This paper focuses on classifying Russian trolls vs. “normal”/control accounts on Twitter. They pay particular attention to Russian attempts to interfere with the 2016 U.S. Election, and create a machine learning model to correctly predict Russian troll accounts from non-troll accounts with high accuracy.
- [Analyzing the Digital Traces of Political Manipulation: The 2016 Russian Interference Twitter Campaign](https://arxiv.org/pdf/1802.04291.pdf): In this paper, researchers used machine learning techniques to "investigate the role and effects of misinformation, using the content produced by Russian Trolls on Twitter as a proxy for misinformation." They specifically looked at both liberal and conservative media outlets, and particularly focused on "users who re-shared the
posts produced on Twitter by the Russian troll accounts publicly disclosed by U.S. Congress investigation" of 2016 election interference.
- [For Whom the Bot Tolls: A Neural Networks Approach to Measuring Political Orientation of Twitter Bots in Russia](https://doi.org/10.1177/2158244019827715): This paper uses neural networks to classify tweets from Russian accounts as being pro-regime, anti-regime, or neutral. Specifically, the researchers used a “deep feedforward neural network (multilayer perceptron [MLP]) that uses a wide range of textual features including words, word pairs, links, mentions, and hashtags to separate four contextually relevant types of bots: pro-Kremlin, neutral/other, pro-opposition, and pro-Kiev.” The results were “high-confidence predictions for most observations”. 
- [Deep Neural Networks for Bot Detection](https://doi.org/10.1016/j.ins.2018.08.019): This paper uses neural networks with a contextual long short-term memory (LSTM) architecture that looks at both content and metadata from Twitter accounts to identify bots among real human users. The authors also used synthetic minority oversampling in order to create a large dataset for training. 
