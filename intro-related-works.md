---
permalink: /intro-related-works/
---

# Introduction and Related Works Draft
Jenna Brandt and Erin Puckett

## Introduction
During the last two presidential elections in the U.S., the impact of Russian disinformation on Twitter has created a need for effective solutions for finding and removing the bots used to propagate disinformation. As such, we are investigating Twitter usage among Democratic and Republican politicians as well as tweets created by Russian bots, as the bots often try to imitate politicians’ rhetoric. We have created a ML algorithm to effectively categorize the author of a tweet as either a Democrat, a Republican, or a bot. 

In addition to being extremely important, finding a solution to this issue is very difficult because there are many features of tweets that could be used to determine whether they were written by bots or real people (message length, frequency, content, time tweeted, number of followers, retweets, etc.). Another approach could be to look at account-level details rather than tweet-level details. Furthermore, often those behind the bots (especially those in cases like this of state-sponsored disinformation campaigns) closely study their targets in order to appear as similar as possible. There is an active intent to fool, and that makes tweets hard to tell apart, particularly as some U.S. politicians themselves have embraced disinformation. With all of that in mind, we decided to focus our algorithm on the content of the tweets themselves, rather than other features. 

We found that it is difficult to assess why our algorithm categorizes certain tweets as it does. The nature of the input with its large vocabulary of words could be part of the reason - the algorithm might frequently encounter new words in new situations it has not trained on. That said, perhaps other solutions would better approach this issue. For example, bot tweets are shorter than politicians’ tweets, so perhaps instead of looking at the actual language used in the tweets, our algorithm might actually categorize tweets based on length. Regardless, our results using a content-based algorithm show moderate success. 

Our algorithm correctly categorizes tweets as Democratic politician, Republican politician, or bot X% of the time. Interestingly, the algorithm had more trouble distinguishing between Republican and bot tweets than Democratic and bot tweets, which points to both the divided nature of American politics, but also the difficulties Twitter would face in deploying an algorithm like ours. Republican tweets miscategorized as Russian bot tweets would anger conservatives. However, the moderate level of success means that social media companies could try other approaches like ours to flag tweets that appear to come from sources who might intend to disrupt American democracy in some way, which would mean that the public (general users of social media) would not be subject to potentially very harmful messaging. 

When conducting this research, we kept in mind the ethical implications of categorizing tweets as “bots” rather than real people. Freedom of speech is a common concern in today’s use of social media. Social media companies purport to be simply a platform for sharing one’s thoughts, and by classifying tweets as bots, and thus potentially misclassifying real human tweets as bots, we risk dismissing their thoughts as not real and part of a malicious campaign to harm American democracy. While the need to identify disinformation made this research worthwhile, we were aware of the potential negative implications of our classifications. 


## Related Works
There has been other work done by researchers involving machine learning and Twitter, specifically with a focus on politicians. We describe several closely related works as follows, and then we note how our research differs from others’ works. A University of Michigan and Georgia Tech [paper](https://arxiv.org/pdf/1901.11162.pdf) from 2018 focuses on classifying Russian trolls vs. “normal”/control accounts on Twitter. They pay particular attention to Russian attempts to interfere with the 2016 U.S. Election, and create a machine learning model to correctly predict Russian troll accounts from non-troll accounts with high accuracy. This paper, unlike ours, does not address American politicians specifically or make any distinctions between Republicans and Democrats.

Another [paper](https://arxiv.org/pdf/1802.04291.pdf), from USC researchers and presented at WWW in 2018, addresses misinformation. In this paper, the researchers used machine learning techniques to “investigate the role and effects of misinformation, using the content produced by Russian Trolls on Twitter as a proxy for misinformation.” They specifically looked at both liberal and conservative media outlets, and particularly focused on “users who re-shared the posts produced on Twitter by the Russian troll accounts publicly disclosed by U.S. Congress investigation” of 2016 election interference. This paper does not focus on actual politicians’ tweets but instead focuses on private citizen retweets; we will focus on the tweets of politicians.

An additional [paper](https://journals.sagepub.com/doi/pdf/10.1177/2158244019827715), from NYU and published in 2019, uses neural networks to classify tweets from Russian accounts as being pro-regime, anti-regime, or neutral. Specifically, the researchers used a “deep feedforward neural network (multilayer perceptron or MLP) that uses a wide range of textual features including words, word pairs, links, mentions, and hashtags to separate four contextually relevant types of bots: pro-Kremlin, neutral/other, pro-opposition, and pro-Kiev.” The results were “high-confidence predictions for most observations”. This paper focuses on only Russian accounts and does not investigate American politicians, compared to our work which concentrates on Russian-produced tweets that relate to events in United States politics as well as American politicians.

Finally, a [paper](https://arxiv.org/pdf/1802.04289.pdf) from 2018 involving researchers from USC and the Indian Institute of Technology uses neural networks with a contextual long short-term memory (LSTM) architecture that looks at both content and metadata from Twitter accounts to identify bots among real human users. The authors also used synthetic minority oversampling in order to create a large dataset for training. Our paper will address a similar issue involving classification of bot tweets versus real human user tweets, but with a focus on political speech involving Russian bots and American Democrat and Republican politicians. Thus, there are a variety of papers that have investigated political tweets, bot tweets, Russian-produced tweets, and differences between bot and real human tweets, but none have specifically looked to distinguish Russian bots, American Republican politicians, and American Democrat politicians. We hope this paper brings some insight into the area of political tweets and Russian interference in American political discourse on Twitter.

## Bibliography

A. Badawy, E. Ferrara and K. Lerman, "Analyzing the Digital Traces of Political Manipulation: The 2016 Russian Interference Twitter Campaign," 2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), Barcelona, Spain, 2018, pp. 258-265, doi: 10.1109/ASONAM.2018.8508646.

Im, J., Chandrasekharan, E., Sargent, J., Lighthammer, P., Denby, T., Bhargava, A., Hemphill, L., Jurgens, D., & Gilbert, E. (2020). Still out there: Modeling and Identifying Russian Troll Accounts on Twitter. 12th ACM Conference on Web Science. https://doi.org/10.1145/3394231.3397889

Kudungunta, S. & Ferrara, E. (2018). Deep Neural Networks for Bot Detection. Cornell University. https://doi.org/10.1016/j.ins.2018.08.019 

Stukal, D., Sanovich, S., Tucker, J. A., & Bonneau, R. (2019). For Whom the Bot Tolls: A Neural Networks Approach to Measuring Political Orientation of Twitter Bots in Russia. SAGE Open. https://doi.org/10.1177/2158244019827715
